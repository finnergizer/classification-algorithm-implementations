<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"></head><body class="c38"><p class="c19 c14 c20 title" id="h.fs8bgu7nyu9q"><span>CSI5149 Project 2 Report - Classification</span></p><p class="c19 c14"><span>Shaughn Finnerty</span></p><p class="c19 c14"><span>6300433</span></p><p class="c19 c14"><span>March 27, 2016</span></p><p class="c3"><span></span></p><p class="c19 c32"><span class="c15"><a class="c23" href="#h.5vhb0olotq4i">Running Instructions</a></span></p><p class="c19 c36"><span class="c15"><a class="c23" href="#h.q6joo8g7jalu">Install Dependencies</a></span></p><p class="c19 c36"><span class="c15"><a class="c23" href="#h.16mwojlb5u8o">Running Instructions</a></span></p><p class="c19 c32"><span class="c15"><a class="c23" href="#h.bfjk5ikdl62d">Description &amp; Discussion of Model &amp; Algorithms</a></span></p><p class="c19 c36"><span class="c15"><a class="c23" href="#h.e70rqjtbdvc0">K-Nearest-Neighbour</a></span></p><p class="c19 c36"><span class="c15"><a class="c23" href="#h.nmeclzmdbte2">Gaussian Naive Bayes</a></span></p><p class="c19 c36"><span class="c15"><a class="c23" href="#h.tz86zfd6k1i">Kernel Density Estimate w/ Naive Bayes</a></span></p><p class="c19 c36"><span class="c15"><a class="c23" href="#h.ny6xd22poiqt">Logistic Regression</a></span></p><p class="c19 c32"><span class="c15"><a class="c23" href="#h.929f25u6l3w6">Results</a></span></p><p class="c19 c32"><span class="c15"><a class="c23" href="#h.z7b0urula70d">References</a></span></p><p class="c3"><span></span></p><h1 class="c19 c20" id="h.5vhb0olotq4i"><span>Running Instructions</span></h1><h2 class="c19 c20" id="h.q6joo8g7jalu"><span>Install Dependencies</span></h2><p class="c3"><span></span></p><p class="c19"><span class="c25">To run this project, you will need the following dependencies install</span></p><p class="c3"><span class="c25"></span></p><ul class="c27 lst-kix_ujvhvsh96a1z-0 start"><li class="c19 c33"><span class="c17">Python 2.7</span></li><li class="c19 c33"><span class="c17">numpy</span><span class="c25">&nbsp;</span><span class="c25">for matrix operations (i.e. computing the transpose of feature matrices and parameter matrices)</span></li></ul><ul class="c27 lst-kix_ujvhvsh96a1z-1 start"><li class="c19 c30"><span class="c25">`pip install numpy`</span></li><li class="c19 c30"><span class="c25">http://docs.scipy.org/doc/numpy-1.10.1/user/install.html</span></li></ul><ul class="c27 lst-kix_ujvhvsh96a1z-0"><li class="c19 c33"><span class="c17">scipy </span><span class="c25">for optimization methods (i.e. minimizing the cost function in the logistic regression classifier)</span></li></ul><ul class="c27 lst-kix_ujvhvsh96a1z-1 start"><li class="c19 c30"><span class="c25">`pip install scipy`</span></li></ul><ul class="c27 lst-kix_ujvhvsh96a1z-0"><li class="c19 c33"><span class="c28">tabulate </span><span>for pretty printing CSV result files</span></li></ul><ul class="c27 lst-kix_ujvhvsh96a1z-1 start"><li class="c19 c30"><span>`pip install tabulate`</span></li></ul><p class="c3"><span class="c25"></span></p><p class="c19"><span class="c25">As seen the pip package manager for python is the best tool to install these dependencies. This package manager can be installed by following the instructions here @ https://pip.pypa.io/en/stable/installing/ (downloading a python file and executing it)</span></p><p class="c3"><span class="c25"></span></p><p class="c19"><span class="c17">When this is downloaded you can use the `requirements.txt` file to handle all the dependences that need to be installed by running the following command from within the project directory:</span></p><p class="c3"><span class="c25"></span></p><p class="c19"><span class="c25">`pip install -r requirements.txt`</span></p><p class="c3"><span class="c25"></span></p><p class="c19"><span class="c25">If you have already ran the pip install commands for each dependency separately, there is no need to perform this command.</span></p><p class="c3"><span></span></p><p class="c19"><span class="c12">Note: We have used numpy and scipy only for the functions that would otherwise be available natively in MatLab (transposition, minimization) and requested permission from the professor before attempt. These are only used in the logistic regression classifier. We have not utilized pre-existing classification algorithms from libraries like Scikit-Learn.</span></p><p class="c3"><span></span></p><h2 class="c19 c20" id="h.16mwojlb5u8o"><span>Running Instructions</span></h2><p class="c3"><span class="c25"></span></p><p class="c19"><span class="c25">To run the project and generate results for the K-Nearest-Neighbour classifier, run the following command:</span></p><p class="c3"><span class="c25"></span></p><p class="c19"><span class="c25">`python run.py --knn`</span></p><p class="c3"><span class="c25"></span></p><p class="c19"><span class="c25">This will train and test the K Nearest Neighbour classifier on all datasets. Furthermore, you can evaluate the other classifiers by running:</span></p><p class="c3"><span class="c25"></span></p><p class="c19"><span class="c25">`python run.py --gnb`</span></p><p class="c3"><span class="c25"></span></p><p class="c19"><span class="c25">`python run.py --kde`</span></p><p class="c3"><span class="c25"></span></p><p class="c19"><span class="c25">`python run.py --lr`</span></p><p class="c3"><span class="c25"></span></p><p class="c19"><span class="c25">To run the Gaussian Naive Bayes classifier, the Kernel Density Estimate Naive Bayes classifier, and the Logistic regression classifier, respectively. To run ALL of the classifiers one after the other, use the command:</span></p><p class="c3"><span class="c25"></span></p><p class="c19"><span class="c25">`python run.py --all`</span></p><p class="c3"><span class="c25"></span></p><p class="c19"><span class="c25">The project has come with the CSV results in the `./results` directory in case you decide not to run them all (as some classifiers will take time to train and test, especially on the larger datasets).</span></p><p class="c3"><span></span></p><p class="c19"><span class="c12">Note: The Gaussian Naive Bayes algorithm will execute in the fastest time. You may will need to wait a fairly extended time for the other algorithms to train and test their models.</span></p><h1 class="c19 c20" id="h.bfjk5ikdl62d"><span>Description &amp; Discussion of Model &amp; Algorithms</span></h1><p class="c3"><span></span></p><h2 class="c19 c20" id="h.e70rqjtbdvc0"><span>K-Nearest-Neighbour</span></h2><p class="c3"><span></span></p><p class="c19"><span>With K-Nearest-Neighbour, the initial model is created from the training data (e.g. 80% of the observations in the provided dataset) by simply transforming the matrix of n features * m observations/items to an easily accessible list/array of vectors, each of which containing the data points for each m features, and the class to which the observation belongs. &nbsp;While there is no parameters to be learned in this model (since it is purely memory-based), the transformation of data allows the list of vectors to be iterated for every classification required in the test phase. </span></p><p class="c3"><span></span></p><p class="c19"><span>To classify a new observation with &ldquo;unknown&rdquo; class, we simply take the feature values for that item in a vector and compute the euclidian distance (note: other distance measures can be used) to the vectors initially created for each observed/training item available. We then sort the list of vectors based on increasing distance. The intuition behind this is that vectors in the training observations that have feature values closer (smaller distance) to those in the unknown item to be classified are likely to have a similar class to the unknown. We then take the first k closest training items (e.g. in our simulations we chose 51), and count the items that belong to class_1 and those that belong to class_2. The class having the most items in the k nearest neighbours is then assigned to be the class of the unknown item (majority vote). Note: Since we are using binary classification, it is useful to choose an odd K to avoid a case where there are an equal amount of items belong to both classes in the first k neighbours.</span></p><p class="c3"><span></span></p><p class="c19"><span>As you will see, this model and algorithm led to the best classification results, achieving &gt; 90% accuracy on all datasets. As such I chose to highlight this algorithm as the </span><span class="c28">best algorithm</span><span>&nbsp;for the project as it provided the best accuracy.</span></p><p class="c3"><span></span></p><p class="c19"><span>The one shortfall was that on large datasets (with large amounts of neighbours to compare to and distances to compute), the computation can take a long time. In future work it would be interesting to experiment with feature selection. That is, as part of the training process, we could identify features that have the most &ldquo;influence&rdquo; when determining the class, and use only these features when computing the distance. Although this would not reduce the amount of neighbours that need to be compared, it would reduce the amount of computations that need to occur when calculating the euclidian distance between the vectors as we would only need to account for the best selected features.</span></p><p class="c3"><span></span></p><p class="c19"><span>Before implementing the KNN classifier, other models/algorithms were implemented in hopes to achieve desired accuracy, but did not prove to perform as well as KNN. However, it is a useful exercise to report on the models/algorithms implemented.</span></p><h2 class="c19 c20" id="h.nmeclzmdbte2"><span>Gaussian Naive Bayes</span></h2><p class="c3"><span></span></p><p class="c19"><span>In this generative model/algorithm, we use a Bayesian approach to produce a function: </span><img src="images/image00.png"><span>&nbsp;where X is the observed item, and Y is the class. This is achieved in the training phase by assuming that each feature in each class in the training items are identically and independently distributed according to a Gaussian Distribution. To create this model, we first calculate the mean and standard deviation for each feature in each class (using values observed from each training item in their respective class). &nbsp;With these parameters, we can then classify/test new unknown items by computing the product of the values of the Gaussian Probability Density Function for each feature in each class. &nbsp;For example to compute the probability that the unknown item X belonged to class 1, we calculated:</span></p><p class="c3"><span></span></p><p class="c19 c14"><img src="images/image01.png"></p><p class="c3 c14"><span></span></p><p class="c19 c14"><span>Where N is the amount of features which were all assumed to be iid according to a Gaussian distribution for each class.</span></p><p class="c3 c14"><span></span></p><p class="c19"><span>A similar probability was calculated for class 2. Of the two, the class with the highest probability calculated was the class to which the unknown item was assigned.</span></p><p class="c3"><span></span></p><h2 class="c19 c20" id="h.tz86zfd6k1i"><span>Kernel Density Estimate w/ Naive Bayes</span></h2><p class="c3"><span></span></p><p class="c19"><span>It was thought that the features might not be distributed normally, so we hoped that we could compute a Kernel Density Estimate of the continuous distribution for each feature in each class might improve the performance. This was performed by computing the average of Gaussians with mean set to every possible continuous value for a given feature observed in the training data (so multiple Gaussian density functions had to be evaluated for each feature, rather than a single one using the computed mean and standard deviation in training). &nbsp;That is, given a value </span><img src="images/image02.png"><span>, its density estimate for class 1 became:</span></p><p class="c3"><span></span></p><p class="c14 c19"><img src="images/image03.png"></p><p class="c19 c14"><span>Where M is the amount of observations for the feature whose continuous distribution is being estimated with a Gaussian kernel.</span></p><p class="c3"><span></span></p><p class="c19"><span>This density estimate was then used in the product computed similar to above in the Gaussian Naive bayes classifier for each feature to compute a probability for each class. The selected class was determined by a similar distinction (i.e. the product of estimated distribution functions that evaluated to provide the highest probability). This technique to use a kernel density estimate of the continuous distribution for each feature was inspired by the Flexible Naive Bayes model presented by John and Langley (1995).</span></p><p class="c3"><span></span></p><p class="c19"><span>Unfortunately, we did not see any noticeable increase in performance using this kernel density estimate for the distribution of each feature in Naive Bayes. One possible hypothesis is that the data for each feature was in fact normally distributed, and thus, the estimation yielded a distribution that was very similar. This would explain why the accuracy results are very similar between the tests run for each algorithm.</span></p><p class="c3"><span></span></p><p class="c19"><span>The results from this algorithm did not show any significant benefit to using the algorithm. In fact, since we had to compute Gaussians for every feature value, and store these feature values as parameters (John and Langley, 1995), the increased storage and processing required made it less favorable than the Gaussian Naive Bayes for this particular problem (where we just had to store and process the initially computed expected values and standard deviations for each feature).</span></p><h2 class="c19 c20" id="h.ny6xd22poiqt"><span>Logistic Regression </span></h2><p class="c3"><span></span></p><p class="c19"><span>The logistic regression classifier allowed us to experiment with creating a discriminative classifier. Compared to a generative classifier with the Naive Bayes, this models the conditional probability </span><img src="images/image04.png"><span>of a class y given an observed item to be classified x.</span></p><p class="c3"><span></span></p><p class="c19"><span>In a logistic regression classifier, our hypothesis representation is defined using the sigmoid function with the product of our transposed parameter vector &nbsp;and feature vector as:</span></p><p class="c3 c14"><span class="c11"></span></p><p class="c19 c14"><img src="images/image05.png"></p><p class="c3"><span></span></p><p class="c19"><span>And since we are solving a binary classification, our model then becomes:</span></p><p class="c3"><span></span></p><p class="c19 c14"><img src="images/image06.png"></p><p class="c19 c14"><img src="images/image07.png"></p><p class="c3 c14"><span></span></p><p class="c19"><span>To fit/estimate the parameters, </span><img src="images/image08.png"><span>, for our hypothesis function, we define a cost function that will be minimized using the training data. By minimizing this cost function over all parameters, &nbsp;we get the parameters least amount of error for our hypothesis function on our training. Depending on the true value of the class (y) for a given training item, the cost function is defined like so:</span></p><p class="c3"><span></span></p><h1 class="c19 c14 c20" id="h.nwl3kyokuiy"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 391.50px; height: 58.03px;"><img alt="" src="images/image12.png" style="width: 391.50px; height: 58.03px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></h1><p class="c19"><span class="c28">As you can see, the cost function increases when our hypothesis function provides an hypothesized class value that is further away from the true (known) class of the item.</span></p><p class="c3"><span></span></p><p class="c19"><span>This cost function is used on all items in the set of training data to estimate all parameters </span><img src="images/image09.png"><span>&nbsp;for each feature. &nbsp;When training, we use the `scipy.optimize.fmin` method to minimize the following function and provide the minimized parameter vector to be used for further classification in the testing phase:</span></p><p class="c3"><span></span></p><p class="c19 c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 321.00px; height: 59.03px;"><img alt="" src="images/image11.png" style="width: 321.00px; height: 59.03px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3 c14"><span></span></p><p class="c19 c14"><span>Where m is the number of items in the training dataset.</span></p><p class="c3 c35"><span></span></p><p class="c19 c35"><span>The `scipy.optimize.fmin` function was useful for minimizing the above function and determining the best-fit parameters. This function minimizes a given function using the downhill simplex algorithm (only using function values, not derivatives or second derivatives). A useful exercise would be to implement the gradient descent method to minimize this function. However, for the sake of time, the provided `fmin` algorithm was used.</span></p><p class="c3 c35"><span></span></p><p class="c3 c35"><span></span></p><p class="c19 c35"><span>One problem that we ran into on larger datasets is that when minimizing, the parameters would converge to a point where the hypothesis function would produce 0 values within the logarithmic function used in the cost function. To account for this domain error, we added an very small epsilon value (</span><img src="images/image10.png"><span>) to the cost function so that it would not raise an error during execution. </span></p><p class="c3"><span></span></p><h1 class="c19 c20" id="h.929f25u6l3w6"><span>Results</span></h1><p class="c3"><span></span></p><p class="c19"><span>Each table provides a summary of testing in a 4:1 ratio of training items to testing items on the datasets provided for this project.</span></p><p class="c3"><span></span></p><p class="c19"><span>We have ordered the results based on which classifier performed best in the experiments.</span></p><p class="c3"><span></span></p><a id="t.25dc15c7d56ff40af799c6c3afa80997f79b396a"></a><a id="t.0"></a><table class="c31"><tbody><tr class="c39"><td class="c24" colspan="7" rowspan="1"><p class="c34 c19 c14"><span class="c2">Results from K-Nearest-Neighbour Classifier Training &amp; Testing, K=51</span></p></td></tr><tr class="c1"><td class="c18" colspan="1" rowspan="1"><p class="c6"><span class="c2">Dataset Name</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c6"><span class="c2">Total Items in Dataset</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c6"><span class="c2"># Of Items In Training Dataset</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c6"><span class="c2"># Of Items In Test Dataset</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c6"><span class="c2">Correct Predictions</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c6"><span class="c2">Incorrect Predictions</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c6"><span class="c2">Accuracy</span></p></td></tr><tr class="c1"><td class="c18" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d3_k2_saved1.mat</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c7"><span class="c2">2000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">1600</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">400</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">381</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">19</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.9525</span></p></td></tr><tr class="c1"><td class="c18" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d3_k2_saved2.mat</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c7"><span class="c2">2000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">1600</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">400</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">394</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">6</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.985</span></p></td></tr><tr class="c1"><td class="c18" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d3_k2_saved3.mat</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c7"><span class="c2">2000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">1600</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">400</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">399</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">1</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.9975</span></p></td></tr><tr class="c1"><td class="c18" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d4_k3_saved1.mat</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c7"><span class="c2">2000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">1600</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">400</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">375</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">25</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.9375</span></p></td></tr><tr class="c1"><td class="c18" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d4_k3_saved2.mat</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c7"><span class="c2">2000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">1600</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">400</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">363</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">37</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.9075</span></p></td></tr><tr class="c1"><td class="c18" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d5_k3_saved1.mat</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c7"><span class="c2">2000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">1600</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">400</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">389</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">11</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.9725</span></p></td></tr><tr class="c1"><td class="c18" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d5_k3_saved2.mat</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c7"><span class="c2">2000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">1600</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">400</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">373</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">27</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.9325</span></p></td></tr><tr class="c1"><td class="c18" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d99_k50_saved1.mat</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c7"><span class="c2">3960</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">3168</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">792</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">789</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">3</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.99621</span></p></td></tr><tr class="c1"><td class="c18" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d99_k50_saved2.mat</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c7"><span class="c2">3960</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">3168</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">792</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">790</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">2</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.99747</span></p></td></tr><tr class="c1"><td class="c18" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d99_k60_saved1.mat</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c7"><span class="c2">3960</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">3168</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">792</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">790</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">2</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.99747</span></p></td></tr><tr class="c1"><td class="c18" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d99_k60_saved2.mat</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c7"><span class="c2">3960</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">3168</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">792</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">787</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">5</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.99369</span></p></td></tr></tbody></table><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><a id="t.d803dae3e351c4067ed862929459b44bdd268f14"></a><a id="t.1"></a><table class="c31"><tbody><tr class="c37"><td class="c24" colspan="7" rowspan="1"><p class="c34 c19 c14"><span class="c2">Results from Gaussian Naive Bayes Classifier Training &amp; Testing</span></p></td></tr><tr class="c1"><td class="c9" colspan="1" rowspan="1"><p class="c6"><span class="c2">Dataset Name</span></p></td><td class="c26" colspan="1" rowspan="1"><p class="c6"><span class="c2">Total Items in Dataset</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c6"><span class="c2"># Of Items In Training Dataset</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c6"><span class="c2"># Of Items In Test Dataset</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c6"><span class="c2">Correct Predictions</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c6"><span class="c2">Incorrect Predictions</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c6"><span class="c2">Accuracy</span></p></td></tr><tr class="c1"><td class="c9" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d3_k2_saved1.mat</span></p></td><td class="c26" colspan="1" rowspan="1"><p class="c7"><span class="c2">2000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">1600</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">400</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">333</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">67</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.8325</span></p></td></tr><tr class="c1"><td class="c9" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d3_k2_saved2.mat</span></p></td><td class="c26" colspan="1" rowspan="1"><p class="c7"><span class="c2">2000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">1600</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">400</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">269</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">131</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.6725</span></p></td></tr><tr class="c1"><td class="c9" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d3_k2_saved3.mat</span></p></td><td class="c26" colspan="1" rowspan="1"><p class="c7"><span class="c2">2000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">1600</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">400</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">356</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">44</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.89</span></p></td></tr><tr class="c1"><td class="c9" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d4_k3_saved1.mat</span></p></td><td class="c26" colspan="1" rowspan="1"><p class="c7"><span class="c2">2000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">1600</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">400</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">286</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">114</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.715</span></p></td></tr><tr class="c1"><td class="c9" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d4_k3_saved2.mat</span></p></td><td class="c26" colspan="1" rowspan="1"><p class="c7"><span class="c2">2000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">1600</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">400</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">278</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">122</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.695</span></p></td></tr><tr class="c1"><td class="c9" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d5_k3_saved1.mat</span></p></td><td class="c26" colspan="1" rowspan="1"><p class="c7"><span class="c2">2000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">1600</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">400</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">282</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">118</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.705</span></p></td></tr><tr class="c1"><td class="c9" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d5_k3_saved2.mat</span></p></td><td class="c26" colspan="1" rowspan="1"><p class="c7"><span class="c2">2000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">1600</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">400</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">273</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">127</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.6825</span></p></td></tr><tr class="c1"><td class="c9" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d99_k50_saved1.mat</span></p></td><td class="c26" colspan="1" rowspan="1"><p class="c7"><span class="c2">3960</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">3168</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">792</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">591</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">201</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.74621</span></p></td></tr><tr class="c1"><td class="c9" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d99_k50_saved2.mat</span></p></td><td class="c26" colspan="1" rowspan="1"><p class="c7"><span class="c2">3960</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">3168</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">792</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">578</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">214</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.7298</span></p></td></tr><tr class="c1"><td class="c9" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d99_k60_saved1.mat</span></p></td><td class="c26" colspan="1" rowspan="1"><p class="c7"><span class="c2">3960</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">3168</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">792</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">592</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">200</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.74747</span></p></td></tr><tr class="c1"><td class="c9" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d99_k60_saved2.mat</span></p></td><td class="c26" colspan="1" rowspan="1"><p class="c7"><span class="c2">3960</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">3168</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">792</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">559</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">233</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.70581</span></p></td></tr></tbody></table><p class="c3"><span></span></p><p class="c3"><span></span></p><a id="t.7ca5199fe75be442e89fd2c26a5dd900c65553a8"></a><a id="t.2"></a><table class="c31"><tbody><tr class="c37"><td class="c24" colspan="7" rowspan="1"><p class="c19 c14 c34"><span class="c2">Results from Kernel Density Estimate Naive Bayes Classifier Training &amp; Testing</span></p></td></tr><tr class="c1"><td class="c18" colspan="1" rowspan="1"><p class="c6"><span class="c2">Dataset Name</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c6"><span class="c2">Total Items in Dataset</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c6"><span class="c2"># Of Items In Training Dataset</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c6"><span class="c2"># Of Items In Test Dataset</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c6"><span class="c2">Correct Predictions</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c6"><span class="c2">Incorrect Predictions</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c6"><span class="c2">Accuracy</span></p></td></tr><tr class="c1"><td class="c18" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d3_k2_saved1.mat</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c7"><span class="c2">2000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">1600</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">400</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">326</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">74</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.815</span></p></td></tr><tr class="c1"><td class="c18" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d3_k2_saved2.mat</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c7"><span class="c2">2000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">1600</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">400</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">271</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">129</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.6775</span></p></td></tr><tr class="c1"><td class="c18" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d3_k2_saved3.mat</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c7"><span class="c2">2000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">1600</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">400</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">346</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">54</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.865</span></p></td></tr><tr class="c1"><td class="c18" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d4_k3_saved1.mat</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c7"><span class="c2">2000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">1600</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">400</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">294</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">106</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.735</span></p></td></tr><tr class="c1"><td class="c18" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d4_k3_saved2.mat</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c7"><span class="c2">2000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">1600</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">400</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">279</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">121</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.6975</span></p></td></tr><tr class="c1"><td class="c18" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d5_k3_saved1.mat</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c7"><span class="c2">2000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">1600</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">400</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">289</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">111</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.7225</span></p></td></tr><tr class="c1"><td class="c18" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d5_k3_saved2.mat</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c7"><span class="c2">2000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">1600</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">400</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">280</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">120</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.7</span></p></td></tr><tr class="c1"><td class="c18" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d99_k50_saved1.mat</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c7"><span class="c2">3960</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">3168</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">792</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">586</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">206</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.7399</span></p></td></tr><tr class="c1"><td class="c18" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d99_k50_saved2.mat</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c7"><span class="c2">3960</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">3168</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">792</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">579</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">213</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.73106</span></p></td></tr><tr class="c1"><td class="c18" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d99_k60_saved1.mat</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c7"><span class="c2">3960</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">3168</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">792</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">591</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">201</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.74621</span></p></td></tr><tr class="c1"><td class="c18" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d99_k60_saved2.mat</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c7"><span class="c2">3960</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">3168</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">792</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">561</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">231</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.70833</span></p></td></tr></tbody></table><p class="c3"><span></span></p><p class="c3"><span></span></p><a id="t.ddf3fdb5d1739099d0bf92e32a52808657a2ddaf"></a><a id="t.3"></a><table class="c31"><tbody><tr class="c37"><td class="c24" colspan="7" rowspan="1"><p class="c34 c19 c14"><span class="c2">Results from Logistic Regression Classifier Training &amp; Testing</span></p></td></tr><tr class="c1"><td class="c8" colspan="1" rowspan="1"><p class="c6"><span class="c2">Dataset Name</span></p></td><td class="c21" colspan="1" rowspan="1"><p class="c6"><span class="c2">Total Items in Dataset</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c6"><span class="c2"># Of Items In Training Dataset</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c6"><span class="c2"># Of Items In Test Dataset</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c6"><span class="c2">Correct Predictions</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c6"><span class="c2">Incorrect Predictions</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c6"><span class="c2">Accuracy</span></p></td></tr><tr class="c1"><td class="c8" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d3_k2_saved1.mat</span></p></td><td class="c21" colspan="1" rowspan="1"><p class="c7"><span class="c2">2000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">1600</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">400</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">289</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">111</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.7225</span></p></td></tr><tr class="c1"><td class="c8" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d3_k2_saved2.mat</span></p></td><td class="c21" colspan="1" rowspan="1"><p class="c7"><span class="c2">2000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">1600</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">400</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">288</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">112</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.72</span></p></td></tr><tr class="c1"><td class="c8" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d3_k2_saved3.mat</span></p></td><td class="c21" colspan="1" rowspan="1"><p class="c7"><span class="c2">2000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">1600</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">400</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">281</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">119</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.7025</span></p></td></tr><tr class="c1"><td class="c8" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d4_k3_saved1.mat</span></p></td><td class="c21" colspan="1" rowspan="1"><p class="c7"><span class="c2">2000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">1600</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">400</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">295</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">105</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.7375</span></p></td></tr><tr class="c1"><td class="c8" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d4_k3_saved2.mat</span></p></td><td class="c21" colspan="1" rowspan="1"><p class="c7"><span class="c2">2000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">1600</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">400</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">278</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">122</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.695</span></p></td></tr><tr class="c1"><td class="c8" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d5_k3_saved1.mat</span></p></td><td class="c21" colspan="1" rowspan="1"><p class="c7"><span class="c2">2000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">1600</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">400</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">301</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">99</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.7525</span></p></td></tr><tr class="c1"><td class="c8" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d5_k3_saved2.mat</span></p></td><td class="c21" colspan="1" rowspan="1"><p class="c7"><span class="c2">2000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">1600</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">400</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">292</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">108</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.73</span></p></td></tr><tr class="c1"><td class="c8" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d99_k50_saved1.mat</span></p></td><td class="c21" colspan="1" rowspan="1"><p class="c7"><span class="c2">3960</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">3168</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">792</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">582</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">210</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.73485</span></p></td></tr><tr class="c1"><td class="c8" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d99_k50_saved2.mat</span></p></td><td class="c21" colspan="1" rowspan="1"><p class="c7"><span class="c2">3960</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">3168</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">792</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">589</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">203</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.74369</span></p></td></tr><tr class="c1"><td class="c8" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d99_k60_saved1.mat</span></p></td><td class="c21" colspan="1" rowspan="1"><p class="c7"><span class="c2">3960</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">3168</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">792</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">601</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">191</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.75884</span></p></td></tr><tr class="c1"><td class="c8" colspan="1" rowspan="1"><p class="c6"><span class="c2">classify_d99_k60_saved2.mat</span></p></td><td class="c21" colspan="1" rowspan="1"><p class="c7"><span class="c2">3960</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c7"><span class="c2">3168</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c7"><span class="c2">792</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">597</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c7"><span class="c2">195</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c2">0.75379</span></p></td></tr></tbody></table><p class="c3"><span></span></p><hr style="page-break-before:always;display:none;"><p class="c3"><span></span></p><h1 class="c19 c20" id="h.z7b0urula70d"><span>References</span></h1><p class="c3"><span></span></p><p class="c10"><span>Breheny, P. (n.d.). Kernel density classification The naive Bayes classifier Kernel density classification.</span></p><p class="c10"><span>John, G. H. G., &amp; Langley, P. (1995). Estimating Continuous Distributions in Bayesian Classifiers. </span><span class="c12">IN PROCEEDINGS OF THE ELEVENTH CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE. Montreal, Quebec, Canada</span><span>, </span><span class="c12">1</span><span>, 338&ndash;345. http://doi.org/10.1.1.8.3257</span></p><p class="c10"><span>Learning Classifiers based on Bayes Rule. (n.d.). Retrieved from www.cs.cmu.edu/&sim;tom/mlbook.html.</span></p><p class="c10"><span>Logistic Regression. (n.d.). Retrieved March 24, 2016, from http://www.holehouse.org/mlclass/06_Logistic_Regression.html</span></p><p class="c10 c29"><span></span></p><p class="c3"><span></span></p></body></html>